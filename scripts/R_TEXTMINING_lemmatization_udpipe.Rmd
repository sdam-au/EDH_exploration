---
title: "Lemmatization of the epigraphic text using Udpipe model"
author: "Petra Hermankova"
date: "26/10/2020"
output:
  html_document:
    theme: united
    toc: yes
    toc_float: true
    number_sections: true
    toc_depth: 2
    df_print: paged
---

# Initial setup

## Setup of the environment:

```{r setup, echo=TRUE, message=FALSE}
devtools::install_github("sdam-au/sdam") # loading SDAM custom package, if not working try devtools::install_github("mplex/cedhar", subdir="pkg/sdam")
#devtools::install_github("mplex/cedhar", subdir="pkg/sdam")
library(tidyverse)
library(sdam)
library(jsonlite)
library(leaflet)
library(tidytext)
library(udpipe)
```

## Loading data
1. Load the dataset, if you have Sciencedata.dk credentials

```{r, echo=FALSE}
mycred_secret<- readLines("~/mysecret.txt")
```

```{r, loading data}
resp = request("EDH_text_cleaned_2021-01-21.json", path="/sharingin/648597@au.dk/SDAM_root/SDAM_data/EDH/public", method="GET", cred=mycred_secret)
```

```{r, echo=FALSE}
remove(mycred_secret)
```

2. Make a list and tibble from the request function
```{r}
list_json <- jsonlite::fromJSON(resp)
EDH_tibble <- as_tibble(list_json)
```

3. Display the first 6 records
```{r}
head(EDH_tibble)
```

# Text mining using udpipe package
source: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-annotation.html#udpipe_-_general

## Download model for given language

Models available at `Straka, Milan and Straková, Jana, 2019, Universal Dependencies 2.5 Models for UDPipe (2019-12-06), LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (ÚFAL), Faculty of Mathematics and Physics, Charles University, http://hdl.handle.net/11234/1-3131.`

```{r}
#dl <- udpipe_download_model(language = "ancient_greek")


dl <- udpipe_download_model(udpipe_model_repo = "jwijffels/udpipe.models.ud.2.5" , language = "latin-proiel", model_dir = "../data/") # latin-proiel model

dl2 <- udpipe_download_model(udpipe_model_repo = "jwijffels/udpipe.models.ud.2.5", language = "latin-perseus", model_dir = "../data/") # latin-perseus model

```

## Give the full path to the model and load it to R
```{r}
# udmodel_anc_greek <- udpipe_load_model(file = "ancient_greek-perseus-ud-2.4-190531.udpipe")

# perseus
udmodel_latin <- udpipe_load_model(file = "../data/latin-perseus-ud-2.5-191206.udpipe")

# proiel
udmodel_latin <- udpipe_load_model(file = "../data/latin-proiel-ud-2.5-191206.udpipe")

```

## Anotate the text using UDpipe
```{r}
udpipe_text <- as.data.frame(udpipe_annotate(udmodel_latin, x = EDH_tibble$clean_text_interpretive_word))
str(udpipe_text)
```


## Overview of linguistic word categories
```{r}
table(udpipe_text$upos)
```

```{r}
nouns <- udpipe_text %>% 
  filter(udpipe_text$upos == "NOUN") 
```

```{r}
verbs <- udpipe_text %>% 
  filter(udpipe_text$upos == "VERB") 
```

```{r}
punctuation <- udpipe_text %>% 
  filter(udpipe_text$upos == "PUNCT") 
```

## The most frequent of all word lemmata
```{r}
udpipe_text %>% 
  count(lemma, sort = TRUE) %>%
  filter(n > 1000) %>% 
  mutate(lemma = reorder(lemma, n)) %>%
  print()
```

## The most frequent of nouns lemmata
```{r}
nouns %>% 
  count(lemma, sort = TRUE) %>%
  filter(n > 1000) %>% 
  mutate(lemma = reorder(lemma, n)) %>%
  print()
```

```{r}
verbs %>% 
  count(lemma, sort = TRUE) %>%
  filter(n > 1000) %>% 
  mutate(lemma = reorder(lemma, n)) %>%
  print()
```

```{r}
punctuation %>% 
  count(lemma, sort = TRUE) %>%
  filter(n > 10) %>% 
  mutate(lemma = reorder(lemma, n)) %>%
  print()
```


## Join the entire dataset with the NLP UD pipe output
```{r}
udpipe_text <- udpipe_text %>% 
  mutate(insc_id = (as.numeric(str_replace(doc_id, pattern = "doc", replacement = ""))))

EDH_tibble <- EDH_tibble %>% 
  mutate(id_num = (as.numeric(str_replace(id, pattern = "HD", replacement = ""))))

full_appended <- left_join(EDH_tibble, udpipe_text, by = c("id_num" = "insc_id"))
```

```{r}
full_appended
```

## Making subset for the article 
 - the full dataset is too big for R to be processed as JSON file
```{r}
EDH_selection <- full_appended %>% 
  select(id_num, people, coordinates, not_before, not_after, type_of_inscription_clean, type_of_inscription_certainty, material_clean, type_of_monument_clean, type_of_monument_certainty, province_label_clean, province_label_certainty, findspot_ancient_clean, findspot_ancient_certainty, clean_text_interpretive_word, doc_id, paragraph_id, sentence_id, sentence, token_id, token, lemma, upos, xpos, feats, head_token_id, dep_rel, deps, misc)
```


# Saving to Sciencedata
```{r}
# saving the full dataset
# EDH_lemmatized_json <- jsonlite::toJSON(full_appended, auto_unbox = TRUE)

# saving subset of the dataset
EDH_lemmatized_json <- jsonlite::toJSON(EDH_selection, auto_unbox = TRUE)

# perseus
write(EDH_lemmatized_json, file="../data/EDH_text_lemmatized_perseus_2021-01-21.json")
request("../data/EDH_text_lemmatized_perseus_2021-01-21.json", path="/sharingout/648597@au.dk/SDAM_root/SDAM_data/EDH/public",
        method="PUT", cred=c(mycred_secret[1], mycred_secret[2]))

# proiel
write(EDH_lemmatized_json, file="../data/EDH_text_lemmatized_proeiel_2021-01-21.json")
request("../data/EDH_text_lemmatized_perseus_2021-01-21.json", path="/sharingout/648597@au.dk/SDAM_root/SDAM_data/EDH/public",
        method="PUT", cred=c(mycred_secret[1], mycred_secret[2]))
```

# Remove local copy of the json and the credential before submitting to GitHub
```{r}
file.remove("./EDH_text_lemmatized_2021-01-11.json")
remove(mycred_secret)
```











